# vLLM AS Ollama - IntelliJ AI Assistantì—ì„œ vLLM ì“°ê¸°

ğŸ‘‰ğŸ»ğŸ‘‰ğŸ»ğŸ‘‰ğŸ» [ğŸ“ŒğŸ‡°ğŸ‡· í•œêµ­ì–´ README](README_ko.md) | [ğŸ“ŒğŸ‡ºğŸ‡¸ English README](README.md) ğŸ‘ˆğŸ»ğŸ‘ˆğŸ»ğŸ‘ˆğŸ»

<img src="https://github.com/user-attachments/assets/97dfec6d-20e0-4354-b6bc-b3588de40ced" width="400">

[![License: WTFPL](https://img.shields.io/badge/License-WTFPL-brightgreen.svg)](http://www.wtfpl.net/)
[![GitHub stars](https://img.shields.io/github/stars/daanta-real/vllm-as-ollama?style=social)](https://github.com/daanta-real/vllm-as-ollama/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/daanta-real/vllm-as-ollama?style=social)](https://github.com/daanta-real/vllm-as-ollama/network/members)

## ğŸš€ ê°œìš”

vLLMì„ Ollamaë¡œ ë‘”ê°‘ì‹œì¼œ IntelliJ AI Assistantì—ì„œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ

This project disguises vLLM as Ollama, making it safely usable within IntelliJ AI Assistant.

<p>$\it{{\color{#DD6565}â€» The\ following\ instructions\ are\ written\ in\ Korean\ only,\ so\ please\ read\ them\ using\ a\ translation\ tool.}}$</p>

## ğŸ’¡ ì™œ í•„ìš”í•œê°€ìš”?

í˜„ì¬ ìµœì‹  ë²„ì „ì˜ IntelliJ AI Assistantì—ì„œëŠ” ì‚¬ìš©ì ì§€ì • LLM ì‹¤í–‰ê¸°ë¥¼ ì—°ê²°ì‹œí‚¬ ìˆ˜ ìˆê²Œ ë˜ì–´ ìˆë‹¤.

ê·¸ëŸ¬ë‚˜ ì´ê±¸ ì…‹íŒ…í•˜ëŠ” ê³¼ì •ì— ë‘ ê°€ì§€ ë¬¸ì œê°€ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.

1. vLLMì€ ëª» ì“´ë‹¤
   - ì¼ë°˜ì ì¸ ì‚¬ìš©ìì˜ ì†Œê·œëª¨ LLM ì‹¤í–‰ê¸°ëŠ” ì‚¬ì–‘ì´ ë„‰ë„‰í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì„±ëŠ¥ì— ë¯¼ê°í•˜ë‹¤.
   - ì´ëŸ´ ë•Œì¼ìˆ˜ë¡ ì¡°ê¸ˆì´ë¼ë„ ë‚˜ì€ í…ìŠ¤íŠ¸ í¼í¬ë¨¼ìŠ¤ë¥¼ ìœ„í•´ Ollamaë³´ë‹¤ëŠ” vLLMì„ ì“°ëŠ” ê²ƒì´ ì¢‹ë‹¤.
   - ê·¼ë° ì •ì‘ IntelliJëŠ” vLLM ì§€ì›ì„ ì•ˆ í•œë‹¤. (ì •í™•íˆëŠ” OpenAPI í˜¸í™˜ ê·œê²©ì˜ LLM ì‹¤í–‰ê¸°ë“¤ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ)
2. https ì…‹íŒ…ì´ í˜ë“¤ë‹¤.
   - LLMì„ ì›ê²© ì ‘ì†í•˜ëŠ” ê²½ìš° ë³´ì•ˆì„ ìœ„í•´ ë°˜ë“œì‹œ HTTPSë¡œ ì—°ê²°í•´ì•¼ í•œë‹¤.
   - ê·¸ëŸ°ë° ì§‘ ë“± ì†Œê·œëª¨ í™˜ê²½ì—ì„œëŠ” ëŒ€ë¶€ë¶„ ë„ë©”ì¸ì´ ì—†ì–´ IP ì£¼ì†Œë¡œ ì—°ê²°í•´ì•¼ í•œë‹¤.
   - ë¶ˆí–‰íˆë„, Let's Encrypt ê°™ì€ ë¬´ë£Œ ì¸ì¦ ê¸°ê´€ì—ì„œëŠ” ì´ëŸ° IP ì£¼ì†Œì—ëŠ” SSL ë°œê¸‰ì„ ì•ˆ í•´ì¤€ë‹¤. HTTPSë¥¼ ëª» ì“´ë‹¤ëŠ” ì†Œë¦¬ë‹¤.

ì´ì— ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ìƒê¸° ë¬¸ì œë“¤ì„ í•´ê²°í•˜ì—¬, IntelliJì—ì„œ ëˆ„êµ¬ë‚˜ vLLMì„ ì—°ê²°í•˜ê³  ì“¸ ìˆ˜ ìˆê²Œ í•˜ì˜€ë‹¤:
1. vLLMì„ Ollama API Bridgeë¡œ ê°ì‹¸ Ollamaë¡œ ë‘”ê°‘ì‹œí‚¤ê³ , ì´ë¥¼ IntelliJì— Ollamaì¸ ê²ƒì²˜ëŸ¼ ì¸ì‹ì‹œì¼œ vLLMì„ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ í•˜ì˜€ë‹¤.
2. SSH í„°ë„ë§ì„ í†µí•´, HTTP request/responseë¥¼ SSH ë°©ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì†¡ìˆ˜ì‹ í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤.
3. ìƒê¸° ì…‹íŒ…ë“¤ì„ í†µí•©í•˜ì—¬ ë‹¨ì¼ Docker Composeë¡œ êµ¬ì„±, ì…‹íŒ…ê³¼ ì¬ì‚¬ìš©ì„ ì‰½ê³  ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤.

ì´ í”„ë¡œì íŠ¸ëŠ” IntelliJê°€ í•´ë‹¹ ê¸°ëŠ¥ì„ ëª¨ë‘ ì§€ì›í•˜ê²Œ ë˜ì–´ ì´ê²ƒì´ í•„ìš” ì—†ì–´ì§ˆ ë•Œê¹Œì§€ ê³„ì†í•´ì„œ ì—…ë°ì´íŠ¸í•´ ë‚˜ê°ˆ ê²ƒì´ë‹¤.

## âš™ ì•„í‚¤í…ì²˜ êµ¬ì„±

ëŒ€ëµì ìœ¼ë¡œ ì•„ë˜ì˜ ì•„í‚¤í…ì²˜ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŒ:

ì„œë²„ì¸¡[vLLM â†’ Ollama í˜¸í™˜ API â†’ ì„œë²„ì¸¡ SSH] â†’ (SSH ì „ì†¡) â†’ í´ë¼ì¸¡[SSH í„°ë„ë§ â†’ IntelliJ]

ì´ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤:

```mermaid
graph TD
    subgraph Client[Client Side]
        IntelliJ["IntelliJ AI Assistant<br/>(localhost:50247)"]
        Tunnel["SSH Tunnel Interface<br/>(ssh -L 50247:vllm_ollama_bridge:11434)"]
        IntelliJ -- "Ollama API Request" --> Tunnel
        Tunnel -- "Ollama API Response" --> IntelliJ
    end

    subgraph Server[Server Side]
        VLLM["vLLM Server<br/>(Port 8000)"]
        Bridge["vLLM to Ollama Bridge<br/>(FastAPI, Port 11434)"]
        Bastion["SSH Bastion Server<br/>(<server_address>:50247, Public Key Auth)"]
        Bridge -- "HTTP/V1 API Request" --> VLLM
        VLLM -- "HTTP/V1 API Response" --> Bridge
    end

    Tunnel -- "SSH (Public Key Auth) Request" --> Bastion
    Bastion -- "SSH (Public Key Auth) Response" --> Tunnel

    Bastion -- "HTTP Request (via SSH Tunnel)" --> Bridge   
    Bridge -- "HTTP Response (via SSH Tunnel)" --> Bastion
```

## ğŸ“ ë¦¬í¬ì§€í† ë¦¬ íŒŒì¼ êµ¬ì„±

```bash
ğŸ“ /
â”œâ”€ ğŸ“ vllm/                               â–£â–£â–£ vLLMì´ ë‹´ê¸´ Docker ì´ë¯¸ì§€ ì •ì˜ (ìˆœì • vLLMê³¼ ê±°ì˜ ë˜‘ê°™ìŒ)
â”‚   â”œâ”€ ğŸ¬ Dockerfile
â”œâ”€ ğŸ“ vllm_ollama_bridge/                 â–£â–£â–£ python Ollama API ë¸Œë¦¿ì§€ê°€ ë‹´ê¸´ Docker ì´ë¯¸ì§€ ì •ì˜ (vLLMì„ ìœ„í•œ Ollama API ë¸Œë¦¿ì§€ ì¸í„°í˜ì´ìŠ¤)
â”‚   â”œâ”€ ğŸ¬ Dockerfile
â”‚   â””â”€ ğŸ vllm_ollama_bridge_server.py
â”œâ”€ ğŸ“ ssh_bastion/                        â–£â–£â–£ HTTP í†µì‹ ì„ ì•ˆì „í•˜ê²Œ í•´ì£¼ê¸° ìœ„í•œ HTTP to SSH í„°ë„ë§ ì„œë²„
â”‚   â”œâ”€ ğŸ¬ Dockerfile
â”‚   â”œâ”€ (ğŸ”‘ vllm_admin.pub)                â–£â–£â–£ í„°ë„ë§ ì„œë²„ ê³µê°œí‚¤ íŒŒì¼, ê´€ë¦¬ììš© (Docker ë¹Œë“œ ì „ì— ì—¬ëŸ¬ë¶„ì´ ë§Œë“¤ì–´ ì§ì ‘ ë„£ì–´ì•¼ ë¨)    
â”‚   â””â”€ (ğŸ”‘ vllm_service_user.pub)         â–£â–£â–£ í„°ë„ë§ ì„œë²„ ê³µê°œí‚¤ íŒŒì¼, ì‚¬ìš©ììš© (Docker ë¹Œë“œ ì „ì— ì—¬ëŸ¬ë¶„ì´ ë§Œë“¤ì–´ ì§ì ‘ ë„£ì–´ì•¼ ë¨)
â”œâ”€â”€ ğŸ³ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ“‹ ì‹œì‘í•˜ê¸°

### ì „ì œ ì¡°ê±´

* OS: Windows/Linux/MacOS ì¤‘ íƒì¼
* GPU: Nvidia GPU í•„ìˆ˜, [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) ì„¤ì¹˜ í•„ìˆ˜
* Docker: [Docker Desktop](https://www.docker.com/products/docker-desktop/) ì„¤ì¹˜ (Windows/macOS) ë˜ëŠ” [Docker Engine](https://docs.docker.com/engine/install/) ì„¤ì¹˜ (Linux) í•„ìˆ˜
* Model: ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œëœ vLLM í˜¸í™˜ ëª¨ë¸ ë¦¬í¬ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ í•„ìˆ˜

  â€» GGUF ì•ˆë¨. vLLMìš© í˜¸í™˜ ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë°›ì„ ê²ƒ (ì˜ˆ: Llama3-1.5B)
* IntelliJ: Community or IDEA í•„ìš”. í”ŒëŸ¬ê·¸ì¸ìœ¼ë¡œ AI Assistant(JetBrains AI Assistant) ì„¤ì¹˜ í•„ìˆ˜

### ì„¤ì¹˜ ë° ì‹¤í–‰

1. ë¦¬í¬ì§€í† ë¦¬ ë°›ê¸°: ì ë‹¹í•œ í´ë”(ì´í•˜ "ë¦¬í¬ì§€í† ë¦¬ í´ë”"ë¡œ ì¹­í•¨)ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•œë‹¤.
    ```bash
    git clone https://github.com/daanta/vllm-as-ollama.git
    ```

2. SSH ì ‘ì†ìš© í‚¤(ê³µê°œí‚¤ì™€ ë¹„ë°€í‚¤) ìƒì„± ë° ì¤€ë¹„
    ```
    1. ì‚¬ìš©ì í´ë”ë¡œ ì´ë™
    â–£â–£â–£        Windows CMD       â–£â–£â–£
    cd %USERPROFILE%
    â–£â–£â–£    Windows Powershell    â–£â–£â–£
    cd $env:USERPROFILE
    â–£â–£â–£ Linux / MacOS / Git Bash â–£â–£â–£
    cd ~
    
    2. SSH í‚¤ ìƒì„±
    â€» ì¤‘ê°„ì— ì´ê²ƒì €ê²ƒ ë­ ë¬¼ì–´ ë³´ë©´ ë‹¤ ê± Enter ì¹˜ë©´ ì•Œì•„ì„œ ë¨.
    â–£â–£â–£       Windows CMD        â–£â–£â–£
    ssh-keygen -t rsa -b 4096 -m PEM -C "vllm_admin" -f "%USERPROFILE%\.ssh\vllm_admin"
    ssh-keygen -t rsa -b 4096 -m PEM -C "vllm_service_user" -f "%USERPROFILE%\.ssh\vllm_service_user"
    â–£â–£â–£    Windows Powershell    â–£â–£â–£
    ssh-keygen -t rsa -b 4096 -m PEM -C "vllm_admin" -f "$env:USERPROFILE\.ssh\vllm_admin"
    ssh-keygen -t rsa -b 4096 -m PEM -C "vllm_service_user" -f "$env:USERPROFILE\.ssh\vllm_service_user"
    â–£â–£â–£ Linux / MacOS / Git Bash â–£â–£â–£
    ssh-keygen -t rsa -b 4096 -m PEM -C "vllm_admin" -f ~/.ssh/vllm_admin
    ssh-keygen -t rsa -b 4096 -m PEM -C "vllm_service_user" -f ~/.ssh/vllm_service_user
    
    3. ìƒì„±ì— ì„±ê³µí–ˆë‹¤ë©´, vllm_admin, vllm_admin.pub, vllm_service_user, vllm_service_user.pub 4ê°œ íŒŒì¼ì´ ìƒì„±ëœë‹¤.
    ì´í›„, ì´ì¤‘ì—ì„œ vllm_admin.pub, vllm_service_user.pub ë‘ ê°œì˜ íŒŒì¼ì„ ë¦¬í¬ì§€í† ë¦¬ í´ë” ë‚´ì— ìˆëŠ” ssh_bastion í´ë”ë¡œ ë³µì‚¬í•œë‹¤.
    ```


3. `docker-compose.yml` íŒŒì¼ í¸ì§‘: vLLMì—ì„œ ë¶ˆëŸ¬ì˜¬ ëª¨ë¸ì˜ ì´ë¦„ê³¼ ê²½ë¡œë¥¼ ìˆ˜ì • í›„ ì €ì¥í•´ì£¼ë„ë¡ í•œë‹¤.

   `services.vllm_server.build.args.MODEL_NAME` â†’ ëª¨ë¸ëª…ì„ ê¸°ì…

   `services.vllm_server.volumes` â†’ ëª¨ë¸ì˜ ê²½ë¡œë¥¼ ê¸°ì…


4. Docker Compose ë¹Œë“œ: `docker-compose.yml` íŒŒì¼ì´ ìˆëŠ” í´ë”ë¡œ ì´ë™ í›„ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ docker composeë¥¼ ë¹Œë“œí•œë‹¤.
    ```bash
    docker compose build
    ```

5. ì„œë²„ì˜ ì‹¤í–‰: vLLM ì„œë²„ì¸¡ì—ì„œëŠ” ì•„ê¹Œ Docker Composeë¡œ ë¹Œë“œëœ ì„œë²„ë¥¼ ì‹¤í–‰í•œë‹¤.

   ```bash
   docker compose up -d
   ```

6. ì‚¬ìš©ìì¸¡(í´ë¼ì´ì–¸íŠ¸)ì˜ ì‹¤í–‰: SSH í„°ë„ì„ ì¼œë†“ì€ ë‹¤ìŒ, ì´ë¥¼ IntelliJì— ì—°ê²°í•œë‹¤. ì¬ë¶€íŒ… ë•Œë§ˆë‹¤ ëª…ë ¹ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ ì¤˜ì•¼ í•œë‹¤.

   â‘  SSH í„°ë„ì˜ ì‹¤í–‰: ì„œë²„ ì¸¡ SSHì— ì—°ê²°ì„ í•˜ê¸° ìœ„í•œ SSH í„°ë„ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì˜¤í”ˆí•œë‹¤. OpenSSH ë“± SSH í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.

   ```bash
   â–£â–£â–£       Windows CMD        â–£â–£â–£
   start /B ssh -i "%USERPROFILE%\.ssh\vllm_service_user" -N -L 50247:vllm_ollama_bridge:11434 vllm_service_user@<server_address> -p 50247
   â–£â–£â–£    Windows Powershell    â–£â–£â–£
   ssh -i "$env:USERPROFILE\.ssh\vllm_service_user" -N -f -L 50247:vllm_ollama_bridge:11434 vllm_service_user@<server_address> -p 50247
   â–£â–£â–£ Linux / MacOS / Git Bash â–£â–£â–£
   ssh -i ~/.ssh/vllm_service_user -N -f -L 50247:vllm_ollama_bridge:11434 vllm_service_user@<server_address> -p 50247
   ```

   â‘¡ ì—°ê²° í…ŒìŠ¤íŠ¸: curl ëª…ë ¹ì–´ë¡œ HTTP requestë¥¼ ë³´ë‚´ Ollama ë¸Œë¦¿ì§€(ë¡œ ë‘”ê°‘í•œ vLLM)ì´ ì˜ ì‹¤í–‰ë˜ëŠ”ì§€ í™•ì¸í•œë‹¤.

   ```bash
   curl http://localhost:50247/api/tags
   ```

   â‘¢ IntelliJ AI Assistant ì— ì„œë²„ ë“±ë¡ ë° ì—°ê²°
   1) ì„¤ì •ì—ì„œ `Tools > AI Assistant > Models` í•­ëª©ìœ¼ë¡œ ì§„ì…
   2) `Enable Ollama`ì— ì²´í¬ í›„, `http://localhost:50247` ë¥¼ ì…ë ¥

      âš ï¸ ëì— `/`ë¥¼ ì…ë ¥í•˜ë©´ ì•ˆ ë˜ë¯€ë¡œ ì£¼ì˜
   3) `Test Connection` ë²„íŠ¼ì„ ëˆŒëŸ¬, ì˜ ì ‘ì†ë˜ì–´ `âœ… Connected` ë¼ëŠ” ë©”ì„¸ì§€ê°€ ì˜ ëœ¨ëŠ”ì§€ í™•ì¸.
   4) ê³ ìƒí•˜ì…¨ìŠµë‹ˆë‹¤. ë°”ë¡œ ì•„ë˜ `Local Models` ë€ì—ì„œ ëª¨ë¸ ê³ ë¥´ê³  ë°”ë¡œ ì“°ì‹œë©´ ë©ë‹ˆë‹¤.

## ğŸ¤ ê¸°ì—¬

[Issues](https://github.com/daanta-real/vllm-as-ollama/issues) í˜ì´ì§€ì— ê°€ì‹œë©´ ë²„ê·¸ ë¦¬í¬íŠ¸, ê¸°ëŠ¥ ì œì•ˆ ë“±ì„ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¼ë° ì‚¬ì‹¤ ë”±ë”±í•œ ê¸€ ì‹«ì–´í•´ìš”, ê± https://github.com/god/earth/issues ì´ëŸ° ì‹ìœ¼ë¡œ ì˜¤ëŠ˜ì˜ ì¼ê¸°ë‚˜ ì•¼ì‹ ì¶”ì²œ ë­ ê·¸ëŸ° ê¸€ì´ë‚˜ ì¨ì£¼ì‹œë©´ ê°ì‚¬í•´ì˜¤.

## ğŸ“„ ë¼ì´ì„ ìŠ¤

WTFPL ë¼ì´ì„¼ìŠ¤ë¥¼ ì¶©ì‹¤íˆ ë”°ë¦…ë‹ˆë‹¤. ë§ˆìŒëŒ€ë¡œ ë³€í˜•í•´ì„œ ì“°ì‹­ì‹œì˜¤.
